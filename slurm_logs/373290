Current conda environment:
# conda environments:
#
base                     /apps/cent7/anaconda/2024.02
CS587                 *  /home/dshur/.conda/envs/cent7/2024.02-py311/CS587


now reading
train_data.device cpu
Original Model: 5.44368 M parameters
MLP Attention Model: 3.175392 M parameters
startin iters
Original model: step 0: train loss 4.6566, val loss 4.6560
MLP Attention model: step 0: train loss 4.5643, val loss 4.5644
Original model: step 50: train loss 2.5788, val loss 2.5660
MLP Attention model: step 50: train loss 3.1416, val loss 3.1324
Original model: step 100: train loss 2.4663, val loss 2.4540
MLP Attention model: step 100: train loss 3.1457, val loss 3.1303
Original model: step 150: train loss 2.4080, val loss 2.3969
MLP Attention model: step 150: train loss 3.1426, val loss 3.1307
Original model: step 200: train loss 2.3128, val loss 2.3007
MLP Attention model: step 200: train loss 3.1393, val loss 3.1319
Original model: step 250: train loss 2.1168, val loss 2.0888
MLP Attention model: step 250: train loss 3.1456, val loss 3.1331
Original model: step 300: train loss 1.9695, val loss 1.9435
MLP Attention model: step 300: train loss 3.1473, val loss 3.1304
Original model: step 350: train loss 1.8689, val loss 1.8443
MLP Attention model: step 350: train loss 3.1461, val loss 3.1304
Original model: step 400: train loss 1.7860, val loss 1.7651
MLP Attention model: step 400: train loss 2.9948, val loss 2.9825
Original model: step 450: train loss 1.7179, val loss 1.7014
MLP Attention model: step 450: train loss 2.7568, val loss 2.7528
Original model: step 500: train loss 1.6597, val loss 1.6487
MLP Attention model: step 500: train loss 2.6844, val loss 2.6808
Original model: step 550: train loss 1.6271, val loss 1.6201
MLP Attention model: step 550: train loss 2.6393, val loss 2.6372
Original model: step 600: train loss 1.5758, val loss 1.5811
MLP Attention model: step 600: train loss 2.6182, val loss 2.6170
Original model: step 650: train loss 1.5413, val loss 1.5499
MLP Attention model: step 650: train loss 2.5954, val loss 2.5951
Original model: step 700: train loss 1.5063, val loss 1.5208
MLP Attention model: step 700: train loss 2.5751, val loss 2.5747
Original model: step 750: train loss 1.4841, val loss 1.5036
MLP Attention model: step 750: train loss 2.5694, val loss 2.5655
Original model: step 800: train loss 1.4526, val loss 1.4808
MLP Attention model: step 800: train loss 2.5504, val loss 2.5466
Original model: step 850: train loss 1.4311, val loss 1.4640
MLP Attention model: step 850: train loss 2.5376, val loss 2.5359
Original model: step 900: train loss 1.4122, val loss 1.4491
MLP Attention model: step 900: train loss 2.5306, val loss 2.5281
Original model: step 950: train loss 1.3902, val loss 1.4341
MLP Attention model: step 950: train loss 2.5235, val loss 2.5188
Original model: step 1000: train loss 1.3724, val loss 1.4293
MLP Attention model: step 1000: train loss 2.5150, val loss 2.5056
Original model: step 1050: train loss 1.3558, val loss 1.4139
MLP Attention model: step 1050: train loss 2.5072, val loss 2.5022
Original model: step 1100: train loss 1.3403, val loss 1.4014
MLP Attention model: step 1100: train loss 2.4862, val loss 2.4816
Original model: step 1150: train loss 1.3242, val loss 1.3908
MLP Attention model: step 1150: train loss 2.4826, val loss 2.4716
Original model: step 1200: train loss 1.3116, val loss 1.3824
MLP Attention model: step 1200: train loss 2.4826, val loss 2.4809
Original model: step 1250: train loss 1.2990, val loss 1.3690
MLP Attention model: step 1250: train loss 2.4604, val loss 2.4536
Original model: step 1300: train loss 1.2871, val loss 1.3645
MLP Attention model: step 1300: train loss 2.4388, val loss 2.4344
Original model: step 1350: train loss 1.2795, val loss 1.3642
MLP Attention model: step 1350: train loss 2.4217, val loss 2.4082
Original model: step 1400: train loss 1.2647, val loss 1.3547
MLP Attention model: step 1400: train loss 2.3928, val loss 2.3839
Original model: step 1450: train loss 1.2564, val loss 1.3487
MLP Attention model: step 1450: train loss 2.3609, val loss 2.3507
Original model: step 1500: train loss 1.2473, val loss 1.3423
MLP Attention model: step 1500: train loss 2.3303, val loss 2.3141
Original model: step 1550: train loss 1.2389, val loss 1.3404
MLP Attention model: step 1550: train loss 2.3067, val loss 2.2805
Original model: step 1600: train loss 1.2255, val loss 1.3335
MLP Attention model: step 1600: train loss 2.2622, val loss 2.2448
Original model: step 1650: train loss 1.2146, val loss 1.3237
MLP Attention model: step 1650: train loss 2.2281, val loss 2.2047
Original model: step 1700: train loss 1.2119, val loss 1.3243
MLP Attention model: step 1700: train loss 2.1911, val loss 2.1686
Original model: step 1750: train loss 1.2002, val loss 1.3238
MLP Attention model: step 1750: train loss 2.1641, val loss 2.1394
Original model: step 1800: train loss 1.1931, val loss 1.3182
MLP Attention model: step 1800: train loss 2.1413, val loss 2.1057
Original model: step 1850: train loss 1.1811, val loss 1.3092
MLP Attention model: step 1850: train loss 2.1100, val loss 2.0769
Original model: step 1900: train loss 1.1770, val loss 1.3118
MLP Attention model: step 1900: train loss 2.0944, val loss 2.0613
Original model: step 1950: train loss 1.1680, val loss 1.3035
MLP Attention model: step 1950: train loss 2.0762, val loss 2.0382
Original model: step 2000: train loss 1.1650, val loss 1.3105
MLP Attention model: step 2000: train loss 2.0633, val loss 2.0289
Original model: step 2050: train loss 1.1575, val loss 1.2985
MLP Attention model: step 2050: train loss 2.0474, val loss 2.0155
Original model: step 2100: train loss 1.1485, val loss 1.2948
MLP Attention model: step 2100: train loss 2.0409, val loss 2.0066
Original model: step 2150: train loss 1.1445, val loss 1.2957
MLP Attention model: step 2150: train loss 2.0284, val loss 1.9981
Original model: step 2200: train loss 1.1363, val loss 1.2912
MLP Attention model: step 2200: train loss 2.0126, val loss 1.9776
Original model: step 2250: train loss 1.1241, val loss 1.2917
MLP Attention model: step 2250: train loss 1.9981, val loss 1.9657
Original model: step 2300: train loss 1.1249, val loss 1.2889
MLP Attention model: step 2300: train loss 1.9873, val loss 1.9556
Original model: step 2350: train loss 1.1201, val loss 1.2876
MLP Attention model: step 2350: train loss 1.9717, val loss 1.9405
Original model: step 2400: train loss 1.1129, val loss 1.2861
MLP Attention model: step 2400: train loss 1.9545, val loss 1.9254
Original model: step 2450: train loss 1.1085, val loss 1.2864
MLP Attention model: step 2450: train loss 1.9366, val loss 1.9042
Original model: step 2500: train loss 1.1003, val loss 1.2819
MLP Attention model: step 2500: train loss 1.9078, val loss 1.8769
Original model: step 2550: train loss 1.0965, val loss 1.2863
MLP Attention model: step 2550: train loss 1.8868, val loss 1.8589
Original model: step 2600: train loss 1.0894, val loss 1.2797
MLP Attention model: step 2600: train loss 1.8674, val loss 1.8421
Original model: step 2650: train loss 1.0813, val loss 1.2789
MLP Attention model: step 2650: train loss 1.8492, val loss 1.8261
Original model: step 2700: train loss 1.0722, val loss 1.2727
MLP Attention model: step 2700: train loss 1.8359, val loss 1.8125
Original model: step 2750: train loss 1.0716, val loss 1.2759
MLP Attention model: step 2750: train loss 1.8213, val loss 1.8027
Original model: step 2800: train loss 1.0689, val loss 1.2785
MLP Attention model: step 2800: train loss 1.8064, val loss 1.7838
Original model: step 2850: train loss 1.0643, val loss 1.2790
MLP Attention model: step 2850: train loss 1.7898, val loss 1.7728
Original model: step 2900: train loss 1.0634, val loss 1.2802
MLP Attention model: step 2900: train loss 1.7740, val loss 1.7546
Original model: step 2950: train loss 1.0579, val loss 1.2768
MLP Attention model: step 2950: train loss 1.7585, val loss 1.7432
Original model: step 2999: train loss 1.0502, val loss 1.2732
MLP Attention model: step 2999: train loss 1.7474, val loss 1.7322
