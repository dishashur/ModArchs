Current conda environment:
# conda environments:
#
base                     /apps/cent7/anaconda/2024.02
CS587                 *  /home/dshur/.conda/envs/cent7/2024.02-py311/CS587


now reading
train_data.device cpu
Original Model: 5.492832 M parameters
MLP Attention Model: 5.787744 M parameters
startin iters
Original model: step 0: train loss 4.7196, val loss 4.7280
MLP Attention model: step 0: train loss 4.8172, val loss 4.8201
Original model: step 50: train loss 2.5621, val loss 2.5511
MLP Attention model: step 50: train loss 2.5574, val loss 2.5482
Original model: step 100: train loss 2.4809, val loss 2.4721
MLP Attention model: step 100: train loss 2.3867, val loss 2.3733
Original model: step 150: train loss 2.4420, val loss 2.4323
MLP Attention model: step 150: train loss 2.1443, val loss 2.1102
Original model: step 200: train loss 2.4020, val loss 2.3911
MLP Attention model: step 200: train loss 2.0566, val loss 2.0274
Original model: step 250: train loss 2.3405, val loss 2.3249
MLP Attention model: step 250: train loss 2.0120, val loss 1.9828
Original model: step 300: train loss 2.2064, val loss 2.1841
MLP Attention model: step 300: train loss 1.9458, val loss 1.9198
Original model: step 350: train loss 1.9982, val loss 1.9685
MLP Attention model: step 350: train loss 1.8239, val loss 1.7994
Original model: step 400: train loss 1.8729, val loss 1.8439
MLP Attention model: step 400: train loss 1.7342, val loss 1.7206
Original model: step 450: train loss 1.7765, val loss 1.7547
MLP Attention model: step 450: train loss 1.6727, val loss 1.6660
Original model: step 500: train loss 1.6938, val loss 1.6810
MLP Attention model: step 500: train loss 1.6243, val loss 1.6233
Original model: step 550: train loss 1.6271, val loss 1.6222
MLP Attention model: step 550: train loss 1.5853, val loss 1.5884
Original model: step 600: train loss 1.5707, val loss 1.5745
MLP Attention model: step 600: train loss 1.5454, val loss 1.5590
Original model: step 650: train loss 1.5350, val loss 1.5454
MLP Attention model: step 650: train loss 1.5151, val loss 1.5336
Original model: step 700: train loss 1.4934, val loss 1.5104
MLP Attention model: step 700: train loss 1.4863, val loss 1.5117
Original model: step 750: train loss 1.4577, val loss 1.4822
MLP Attention model: step 750: train loss 1.4605, val loss 1.4897
Original model: step 800: train loss 1.4300, val loss 1.4604
MLP Attention model: step 800: train loss 1.4265, val loss 1.4676
Original model: step 850: train loss 1.4024, val loss 1.4420
MLP Attention model: step 850: train loss 1.4111, val loss 1.4564
Original model: step 900: train loss 1.3785, val loss 1.4281
MLP Attention model: step 900: train loss 1.4006, val loss 1.4498
Original model: step 950: train loss 1.3618, val loss 1.4164
MLP Attention model: step 950: train loss 1.3740, val loss 1.4293
Original model: step 1000: train loss 1.3419, val loss 1.4058
MLP Attention model: step 1000: train loss 1.3512, val loss 1.4175
Original model: step 1050: train loss 1.3224, val loss 1.3989
MLP Attention model: step 1050: train loss 1.3381, val loss 1.4021
Original model: step 1100: train loss 1.3031, val loss 1.3789
MLP Attention model: step 1100: train loss 1.3231, val loss 1.3961
Original model: step 1150: train loss 1.2775, val loss 1.3635
MLP Attention model: step 1150: train loss 1.3048, val loss 1.3865
Original model: step 1200: train loss 1.2618, val loss 1.3528
MLP Attention model: step 1200: train loss 1.2952, val loss 1.3831
Original model: step 1250: train loss 1.2475, val loss 1.3508
MLP Attention model: step 1250: train loss 1.2805, val loss 1.3747
Original model: step 1300: train loss 1.2377, val loss 1.3396
MLP Attention model: step 1300: train loss 1.2722, val loss 1.3668
Original model: step 1350: train loss 1.2279, val loss 1.3379
MLP Attention model: step 1350: train loss 1.2569, val loss 1.3587
Original model: step 1400: train loss 1.2100, val loss 1.3269
MLP Attention model: step 1400: train loss 1.2442, val loss 1.3546
Original model: step 1450: train loss 1.2024, val loss 1.3201
MLP Attention model: step 1450: train loss 1.2297, val loss 1.3415
Original model: step 1500: train loss 1.1872, val loss 1.3165
MLP Attention model: step 1500: train loss 1.2240, val loss 1.3392
Original model: step 1550: train loss 1.1790, val loss 1.3130
MLP Attention model: step 1550: train loss 1.2076, val loss 1.3366
Original model: step 1600: train loss 1.1671, val loss 1.3049
MLP Attention model: step 1600: train loss 1.2003, val loss 1.3300
Original model: step 1650: train loss 1.1649, val loss 1.3163
MLP Attention model: step 1650: train loss 1.2062, val loss 1.3330
Original model: step 1700: train loss 1.1451, val loss 1.3031
MLP Attention model: step 1700: train loss 1.1852, val loss 1.3225
Original model: step 1750: train loss 1.1429, val loss 1.3006
MLP Attention model: step 1750: train loss 1.1722, val loss 1.3194
Original model: step 1800: train loss 1.1282, val loss 1.2975
MLP Attention model: step 1800: train loss 1.1671, val loss 1.3130
Original model: step 1850: train loss 1.1164, val loss 1.2914
MLP Attention model: step 1850: train loss 1.1565, val loss 1.3148
Original model: step 1900: train loss 1.1119, val loss 1.2893
MLP Attention model: step 1900: train loss 1.1566, val loss 1.3105
Original model: step 1950: train loss 1.1067, val loss 1.2893
MLP Attention model: step 1950: train loss 1.1375, val loss 1.3031
Original model: step 2000: train loss 1.0909, val loss 1.2818
MLP Attention model: step 2000: train loss 1.1467, val loss 1.3140
Original model: step 2050: train loss 1.0823, val loss 1.2739
MLP Attention model: step 2050: train loss 1.1282, val loss 1.3043
Original model: step 2100: train loss 1.0751, val loss 1.2837
MLP Attention model: step 2100: train loss 1.1205, val loss 1.2994
Original model: step 2150: train loss 1.0689, val loss 1.2807
MLP Attention model: step 2150: train loss 1.1082, val loss 1.2983
Original model: step 2200: train loss 1.0605, val loss 1.2792
MLP Attention model: step 2200: train loss 1.1027, val loss 1.2880
Original model: step 2250: train loss 1.0582, val loss 1.2763
MLP Attention model: step 2250: train loss 1.0954, val loss 1.2929
Original model: step 2300: train loss 1.0508, val loss 1.2729
MLP Attention model: step 2300: train loss 1.0878, val loss 1.2907
Original model: step 2350: train loss 1.0388, val loss 1.2686
MLP Attention model: step 2350: train loss 1.0805, val loss 1.2919
Original model: step 2400: train loss 1.0321, val loss 1.2674
MLP Attention model: step 2400: train loss 1.0730, val loss 1.2920
Original model: step 2450: train loss 1.0284, val loss 1.2738
MLP Attention model: step 2450: train loss 1.0707, val loss 1.2886
Original model: step 2500: train loss 1.0144, val loss 1.2638
MLP Attention model: step 2500: train loss 1.0613, val loss 1.2878
Original model: step 2550: train loss 1.0161, val loss 1.2750
MLP Attention model: step 2550: train loss 1.0590, val loss 1.2832
Original model: step 2600: train loss 1.0059, val loss 1.2723
MLP Attention model: step 2600: train loss 1.0463, val loss 1.2893
Original model: step 2650: train loss 0.9992, val loss 1.2710
MLP Attention model: step 2650: train loss 1.0422, val loss 1.2836
Original model: step 2700: train loss 0.9956, val loss 1.2653
MLP Attention model: step 2700: train loss 1.0383, val loss 1.2825
Original model: step 2750: train loss 0.9887, val loss 1.2680
MLP Attention model: step 2750: train loss 1.0319, val loss 1.2903
Original model: step 2800: train loss 0.9784, val loss 1.2729
MLP Attention model: step 2800: train loss 1.0150, val loss 1.2805
Original model: step 2850: train loss 0.9729, val loss 1.2689
MLP Attention model: step 2850: train loss 1.0198, val loss 1.2834
Original model: step 2900: train loss 0.9648, val loss 1.2703
MLP Attention model: step 2900: train loss 1.0078, val loss 1.2817
Original model: step 2950: train loss 0.9662, val loss 1.2740
MLP Attention model: step 2950: train loss 1.0159, val loss 1.2857
Original model: step 2999: train loss 0.9523, val loss 1.2646
MLP Attention model: step 2999: train loss 0.9962, val loss 1.2845
