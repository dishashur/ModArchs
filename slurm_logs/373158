Current conda environment:
# conda environments:
#
base                     /apps/cent7/anaconda/2024.02
CS587                 *  /home/dshur/.conda/envs/cent7/2024.02-py311/CS587


now reading
train_data.device cpu
Original Model: 5.787744 M parameters
MLP Attention Model: 3.224544 M parameters
startin iters
Original model: step 0: train loss 4.4275, val loss 4.4200
MLP Attention model: step 0: train loss 4.5656, val loss 4.5655
Original model: step 50: train loss 2.5562, val loss 2.5446
MLP Attention model: step 50: train loss 3.1493, val loss 3.1345
Original model: step 100: train loss 2.3862, val loss 2.3740
MLP Attention model: step 100: train loss 3.1457, val loss 3.1325
Original model: step 150: train loss 2.1386, val loss 2.1097
MLP Attention model: step 150: train loss 3.1439, val loss 3.1330
Original model: step 200: train loss 2.0584, val loss 2.0281
MLP Attention model: step 200: train loss 3.1446, val loss 3.1334
Original model: step 250: train loss 2.0111, val loss 1.9790
MLP Attention model: step 250: train loss 3.1420, val loss 3.1290
Original model: step 300: train loss 1.9269, val loss 1.8986
MLP Attention model: step 300: train loss 3.1434, val loss 3.1327
Original model: step 350: train loss 1.7999, val loss 1.7796
MLP Attention model: step 350: train loss 3.1035, val loss 3.0985
Original model: step 400: train loss 1.7187, val loss 1.7035
MLP Attention model: step 400: train loss 2.7787, val loss 2.7714
Original model: step 450: train loss 1.6632, val loss 1.6530
MLP Attention model: step 450: train loss 2.6878, val loss 2.6829
Original model: step 500: train loss 1.6163, val loss 1.6112
MLP Attention model: step 500: train loss 2.6483, val loss 2.6469
Original model: step 550: train loss 1.5694, val loss 1.5798
MLP Attention model: step 550: train loss 2.6280, val loss 2.6243
Original model: step 600: train loss 1.5325, val loss 1.5464
MLP Attention model: step 600: train loss 2.5995, val loss 2.5949
Original model: step 650: train loss 1.5009, val loss 1.5217
MLP Attention model: step 650: train loss 2.5827, val loss 2.5754
Original model: step 700: train loss 1.4764, val loss 1.5024
MLP Attention model: step 700: train loss 2.5764, val loss 2.5683
Original model: step 750: train loss 1.4484, val loss 1.4791
MLP Attention model: step 750: train loss 2.5684, val loss 2.5616
Original model: step 800: train loss 1.4303, val loss 1.4685
MLP Attention model: step 800: train loss 2.5626, val loss 2.5547
Original model: step 850: train loss 1.4127, val loss 1.4599
MLP Attention model: step 850: train loss 2.5572, val loss 2.5494
Original model: step 900: train loss 1.3871, val loss 1.4405
MLP Attention model: step 900: train loss 2.5480, val loss 2.5432
Original model: step 950: train loss 1.3733, val loss 1.4301
MLP Attention model: step 950: train loss 2.5511, val loss 2.5426
Original model: step 1000: train loss 1.3547, val loss 1.4217
MLP Attention model: step 1000: train loss 2.5423, val loss 2.5387
Original model: step 1050: train loss 1.3370, val loss 1.4083
MLP Attention model: step 1050: train loss 2.5366, val loss 2.5282
Original model: step 1100: train loss 1.3287, val loss 1.4015
MLP Attention model: step 1100: train loss 2.5866, val loss 2.5740
Original model: step 1150: train loss 1.3071, val loss 1.3914
MLP Attention model: step 1150: train loss 2.5586, val loss 2.5492
Original model: step 1200: train loss 1.3032, val loss 1.3830
MLP Attention model: step 1200: train loss 2.5372, val loss 2.5248
Original model: step 1250: train loss 1.2856, val loss 1.3741
MLP Attention model: step 1250: train loss 2.5296, val loss 2.5228
Original model: step 1300: train loss 1.2715, val loss 1.3676
MLP Attention model: step 1300: train loss 2.5221, val loss 2.5119
Original model: step 1350: train loss 1.2658, val loss 1.3673
MLP Attention model: step 1350: train loss 2.5208, val loss 2.5097
Original model: step 1400: train loss 1.2512, val loss 1.3545
MLP Attention model: step 1400: train loss 2.5434, val loss 2.5375
Original model: step 1450: train loss 1.2489, val loss 1.3622
MLP Attention model: step 1450: train loss 2.5139, val loss 2.5057
Original model: step 1500: train loss 1.2237, val loss 1.3409
MLP Attention model: step 1500: train loss 2.5120, val loss 2.5041
Original model: step 1550: train loss 1.2167, val loss 1.3418
MLP Attention model: step 1550: train loss 2.5126, val loss 2.5050
Original model: step 1600: train loss 1.2053, val loss 1.3289
MLP Attention model: step 1600: train loss 2.5107, val loss 2.5056
Original model: step 1650: train loss 1.2053, val loss 1.3307
MLP Attention model: step 1650: train loss 2.5067, val loss 2.4976
Original model: step 1700: train loss 1.1863, val loss 1.3227
MLP Attention model: step 1700: train loss 2.5077, val loss 2.4987
Original model: step 1750: train loss 1.1776, val loss 1.3164
MLP Attention model: step 1750: train loss 2.5066, val loss 2.4995
Original model: step 1800: train loss 1.1730, val loss 1.3181
MLP Attention model: step 1800: train loss 2.5011, val loss 2.4916
Original model: step 1850: train loss 1.1583, val loss 1.3101
MLP Attention model: step 1850: train loss 2.4987, val loss 2.4928
Original model: step 1900: train loss 1.1599, val loss 1.3149
MLP Attention model: step 1900: train loss 2.4994, val loss 2.4941
Original model: step 1950: train loss 1.1478, val loss 1.3099
MLP Attention model: step 1950: train loss 2.4957, val loss 2.4871
Original model: step 2000: train loss 1.1328, val loss 1.3037
MLP Attention model: step 2000: train loss 2.5186, val loss 2.5115
Original model: step 2050: train loss 1.1229, val loss 1.2968
MLP Attention model: step 2050: train loss 2.5012, val loss 2.4942
Original model: step 2100: train loss 1.1180, val loss 1.2950
MLP Attention model: step 2100: train loss 2.4940, val loss 2.4854
Original model: step 2150: train loss 1.1053, val loss 1.2942
MLP Attention model: step 2150: train loss 2.4951, val loss 2.4873
Original model: step 2200: train loss 1.1057, val loss 1.2967
MLP Attention model: step 2200: train loss 2.4879, val loss 2.4800
Original model: step 2250: train loss 1.1016, val loss 1.2942
MLP Attention model: step 2250: train loss 2.4859, val loss 2.4775
Original model: step 2300: train loss 1.0905, val loss 1.2864
MLP Attention model: step 2300: train loss 2.4821, val loss 2.4750
Original model: step 2350: train loss 1.0860, val loss 1.2879
MLP Attention model: step 2350: train loss 2.4832, val loss 2.4746
Original model: step 2400: train loss 1.0719, val loss 1.2844
MLP Attention model: step 2400: train loss 2.4860, val loss 2.4776
Original model: step 2450: train loss 1.0720, val loss 1.2819
MLP Attention model: step 2450: train loss 2.4861, val loss 2.4795
Original model: step 2500: train loss 1.0623, val loss 1.2895
MLP Attention model: step 2500: train loss 2.4829, val loss 2.4759
Original model: step 2550: train loss 1.0521, val loss 1.2810
MLP Attention model: step 2550: train loss 2.4764, val loss 2.4666
Original model: step 2600: train loss 1.1081, val loss 1.3205
MLP Attention model: step 2600: train loss 2.4764, val loss 2.4698
Original model: step 2650: train loss 1.0477, val loss 1.2848
MLP Attention model: step 2650: train loss 2.4879, val loss 2.4800
Original model: step 2700: train loss 1.0346, val loss 1.2810
MLP Attention model: step 2700: train loss 2.4684, val loss 2.4609
Original model: step 2750: train loss 1.0367, val loss 1.2743
MLP Attention model: step 2750: train loss 2.4676, val loss 2.4577
Original model: step 2800: train loss 1.0264, val loss 1.2726
MLP Attention model: step 2800: train loss 2.4731, val loss 2.4628
Original model: step 2850: train loss 1.0240, val loss 1.2780
MLP Attention model: step 2850: train loss 2.4630, val loss 2.4548
Original model: step 2900: train loss 1.0155, val loss 1.2781
MLP Attention model: step 2900: train loss 2.4614, val loss 2.4521
Original model: step 2950: train loss 1.0129, val loss 1.2705
MLP Attention model: step 2950: train loss 2.4542, val loss 2.4486
Original model: step 2999: train loss 1.0070, val loss 1.2756
MLP Attention model: step 2999: train loss 2.4535, val loss 2.4442
