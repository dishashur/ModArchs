Current conda environment:
# conda environments:
#
base                     /apps/cent7/anaconda/2024.02
CS587                 *  /home/dshur/.conda/envs/cent7/2024.02-py311/CS587


now reading
train_data.device cpu
Original Model: 5.492832 M parameters
MLP Attention Model: 3.61776 M parameters
startin iters
Original model: step 0: train loss 4.6825, val loss 4.6836
MLP Attention model: step 0: train loss 4.5633, val loss 4.5633
Original model: step 50: train loss 2.5647, val loss 2.5595
MLP Attention model: step 50: train loss 3.1431, val loss 3.1336
Original model: step 100: train loss 2.4880, val loss 2.4787
MLP Attention model: step 100: train loss 3.1454, val loss 3.1346
Original model: step 150: train loss 2.4450, val loss 2.4352
MLP Attention model: step 150: train loss 3.1404, val loss 3.1304
Original model: step 200: train loss 2.4122, val loss 2.3989
MLP Attention model: step 200: train loss 3.1463, val loss 3.1314
Original model: step 250: train loss 2.3659, val loss 2.3483
MLP Attention model: step 250: train loss 3.1392, val loss 3.1294
Original model: step 300: train loss 2.2649, val loss 2.2447
MLP Attention model: step 300: train loss 3.1445, val loss 3.1330
Original model: step 350: train loss 2.0755, val loss 2.0457
MLP Attention model: step 350: train loss 3.1421, val loss 3.1284
Original model: step 400: train loss 1.9389, val loss 1.9062
MLP Attention model: step 400: train loss 3.1446, val loss 3.1337
Original model: step 450: train loss 1.8204, val loss 1.7988
MLP Attention model: step 450: train loss 3.0349, val loss 3.0217
Original model: step 500: train loss 1.7337, val loss 1.7144
MLP Attention model: step 500: train loss 2.7500, val loss 2.7418
Original model: step 550: train loss 1.6593, val loss 1.6505
MLP Attention model: step 550: train loss 2.6652, val loss 2.6614
Original model: step 600: train loss 1.5929, val loss 1.5910
MLP Attention model: step 600: train loss 2.6429, val loss 2.6413
Original model: step 650: train loss 1.5492, val loss 1.5579
MLP Attention model: step 650: train loss 2.6042, val loss 2.6024
Original model: step 700: train loss 1.5012, val loss 1.5200
MLP Attention model: step 700: train loss 2.5884, val loss 2.5805
Original model: step 750: train loss 1.4632, val loss 1.4908
MLP Attention model: step 750: train loss 2.5755, val loss 2.5679
Original model: step 800: train loss 1.4309, val loss 1.4629
MLP Attention model: step 800: train loss 2.5620, val loss 2.5553
Original model: step 850: train loss 1.3971, val loss 1.4383
MLP Attention model: step 850: train loss 2.5541, val loss 2.5442
Original model: step 900: train loss 1.3684, val loss 1.4191
MLP Attention model: step 900: train loss 2.5514, val loss 2.5461
Original model: step 950: train loss 1.3424, val loss 1.4006
MLP Attention model: step 950: train loss 2.5401, val loss 2.5340
Original model: step 1000: train loss 1.3154, val loss 1.3812
MLP Attention model: step 1000: train loss 2.5361, val loss 2.5335
Original model: step 1050: train loss 1.3012, val loss 1.3719
MLP Attention model: step 1050: train loss 2.5297, val loss 2.5213
Original model: step 1100: train loss 1.2870, val loss 1.3631
MLP Attention model: step 1100: train loss 2.5291, val loss 2.5242
Original model: step 1150: train loss 1.2660, val loss 1.3494
MLP Attention model: step 1150: train loss 2.5275, val loss 2.5224
Original model: step 1200: train loss 1.2478, val loss 1.3465
MLP Attention model: step 1200: train loss 2.5261, val loss 2.5195
Original model: step 1250: train loss 1.2341, val loss 1.3400
MLP Attention model: step 1250: train loss 2.5229, val loss 2.5171
Original model: step 1300: train loss 1.2204, val loss 1.3279
MLP Attention model: step 1300: train loss 2.5176, val loss 2.5142
Original model: step 1350: train loss 1.2011, val loss 1.3218
MLP Attention model: step 1350: train loss 2.5110, val loss 2.5046
Original model: step 1400: train loss 1.1871, val loss 1.3135
MLP Attention model: step 1400: train loss 2.5078, val loss 2.5014
Original model: step 1450: train loss 1.1804, val loss 1.3152
MLP Attention model: step 1450: train loss 2.5072, val loss 2.5020
Original model: step 1500: train loss 1.1671, val loss 1.3091
MLP Attention model: step 1500: train loss 2.5050, val loss 2.4958
Original model: step 1550: train loss 1.1513, val loss 1.2989
MLP Attention model: step 1550: train loss 2.5030, val loss 2.4952
Original model: step 1600: train loss 1.1398, val loss 1.2932
MLP Attention model: step 1600: train loss 2.5016, val loss 2.4945
Original model: step 1650: train loss 1.1289, val loss 1.2928
MLP Attention model: step 1650: train loss 2.4940, val loss 2.4863
Original model: step 1700: train loss 1.1135, val loss 1.2792
MLP Attention model: step 1700: train loss 2.4970, val loss 2.4890
Original model: step 1750: train loss 1.1047, val loss 1.2857
MLP Attention model: step 1750: train loss 2.4958, val loss 2.4883
Original model: step 1800: train loss 1.0917, val loss 1.2746
MLP Attention model: step 1800: train loss 2.4911, val loss 2.4862
Original model: step 1850: train loss 1.0860, val loss 1.2780
MLP Attention model: step 1850: train loss 2.4907, val loss 2.4802
Original model: step 1900: train loss 1.0789, val loss 1.2771
MLP Attention model: step 1900: train loss 2.4900, val loss 2.4815
Original model: step 1950: train loss 1.0654, val loss 1.2712
MLP Attention model: step 1950: train loss 2.4836, val loss 2.4736
Original model: step 2000: train loss 1.0580, val loss 1.2719
MLP Attention model: step 2000: train loss 2.4889, val loss 2.4791
Original model: step 2050: train loss 1.0516, val loss 1.2745
MLP Attention model: step 2050: train loss 2.4757, val loss 2.4676
Original model: step 2100: train loss 1.0391, val loss 1.2648
MLP Attention model: step 2100: train loss 2.4719, val loss 2.4630
Original model: step 2150: train loss 1.0302, val loss 1.2689
MLP Attention model: step 2150: train loss 2.4733, val loss 2.4631
Original model: step 2200: train loss 1.0199, val loss 1.2641
MLP Attention model: step 2200: train loss 2.4667, val loss 2.4570
Original model: step 2250: train loss 1.0193, val loss 1.2631
MLP Attention model: step 2250: train loss 2.4671, val loss 2.4586
Original model: step 2300: train loss 1.0028, val loss 1.2619
MLP Attention model: step 2300: train loss 2.4614, val loss 2.4495
Original model: step 2350: train loss 0.9984, val loss 1.2640
MLP Attention model: step 2350: train loss 2.4583, val loss 2.4496
Original model: step 2400: train loss 0.9980, val loss 1.2693
MLP Attention model: step 2400: train loss 2.4540, val loss 2.4452
Original model: step 2450: train loss 0.9864, val loss 1.2645
MLP Attention model: step 2450: train loss 2.4500, val loss 2.4429
Original model: step 2500: train loss 0.9761, val loss 1.2679
MLP Attention model: step 2500: train loss 2.4458, val loss 2.4368
Original model: step 2550: train loss 0.9691, val loss 1.2609
MLP Attention model: step 2550: train loss 2.4438, val loss 2.4326
Original model: step 2600: train loss 0.9622, val loss 1.2608
MLP Attention model: step 2600: train loss 2.4423, val loss 2.4323
Original model: step 2650: train loss 0.9564, val loss 1.2629
MLP Attention model: step 2650: train loss 2.4431, val loss 2.4311
Original model: step 2700: train loss 0.9511, val loss 1.2680
MLP Attention model: step 2700: train loss 2.4269, val loss 2.4164
Original model: step 2750: train loss 0.9401, val loss 1.2638
MLP Attention model: step 2750: train loss 2.4201, val loss 2.4073
Original model: step 2800: train loss 0.9297, val loss 1.2634
MLP Attention model: step 2800: train loss 2.4054, val loss 2.3969
Original model: step 2850: train loss 0.9271, val loss 1.2645
MLP Attention model: step 2850: train loss 2.3958, val loss 2.3827
Original model: step 2900: train loss 0.9200, val loss 1.2697
MLP Attention model: step 2900: train loss 2.3855, val loss 2.3692
Original model: step 2950: train loss 0.9156, val loss 1.2628
MLP Attention model: step 2950: train loss 2.3674, val loss 2.3520
Original model: step 2999: train loss 0.9088, val loss 1.2679
MLP Attention model: step 2999: train loss 2.3515, val loss 2.3358
