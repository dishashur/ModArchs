Current conda environment:
# conda environments:
#
base                     /apps/cent7/anaconda/2024.02
CS587                 *  /home/dshur/.conda/envs/cent7/2024.02-py311/CS587


now reading
train_data.device cpu
Original Model: 5.492832 M parameters
MLP Attention Model: 6.967392 M parameters
startin iters
Original model: step 0: train loss 4.4809, val loss 4.4777
MLP Attention model: step 0: train loss 4.6250, val loss 4.6237
Original model: step 50: train loss 2.5561, val loss 2.5467
MLP Attention model: step 50: train loss 2.5571, val loss 2.5450
Original model: step 100: train loss 2.4755, val loss 2.4653
MLP Attention model: step 100: train loss 2.3924, val loss 2.3792
Original model: step 150: train loss 2.4424, val loss 2.4314
MLP Attention model: step 150: train loss 2.1522, val loss 2.1236
Original model: step 200: train loss 2.3985, val loss 2.3851
MLP Attention model: step 200: train loss 2.0590, val loss 2.0330
Original model: step 250: train loss 2.3423, val loss 2.3276
MLP Attention model: step 250: train loss 2.0026, val loss 1.9768
Original model: step 300: train loss 2.2754, val loss 2.2547
MLP Attention model: step 300: train loss 1.8885, val loss 1.8620
Original model: step 350: train loss 2.1405, val loss 2.1161
MLP Attention model: step 350: train loss 1.7791, val loss 1.7601
Original model: step 400: train loss 1.9682, val loss 1.9395
MLP Attention model: step 400: train loss 1.7036, val loss 1.6917
Original model: step 450: train loss 1.8616, val loss 1.8370
MLP Attention model: step 450: train loss 1.6455, val loss 1.6457
Original model: step 500: train loss 1.7652, val loss 1.7464
MLP Attention model: step 500: train loss 1.6004, val loss 1.5997
Original model: step 550: train loss 1.6884, val loss 1.6775
MLP Attention model: step 550: train loss 1.5547, val loss 1.5657
Original model: step 600: train loss 1.6150, val loss 1.6145
MLP Attention model: step 600: train loss 1.5182, val loss 1.5320
Original model: step 650: train loss 1.5593, val loss 1.5686
MLP Attention model: step 650: train loss 1.4839, val loss 1.5078
Original model: step 700: train loss 1.5162, val loss 1.5321
MLP Attention model: step 700: train loss 1.4516, val loss 1.4855
Original model: step 750: train loss 1.4649, val loss 1.4937
MLP Attention model: step 750: train loss 1.4236, val loss 1.4635
Original model: step 800: train loss 1.4338, val loss 1.4699
MLP Attention model: step 800: train loss 1.3998, val loss 1.4486
Original model: step 850: train loss 1.4010, val loss 1.4462
MLP Attention model: step 850: train loss 1.3775, val loss 1.4318
Original model: step 900: train loss 1.3721, val loss 1.4265
MLP Attention model: step 900: train loss 1.3605, val loss 1.4222
Original model: step 950: train loss 1.3514, val loss 1.4100
MLP Attention model: step 950: train loss 1.3428, val loss 1.4118
Original model: step 1000: train loss 1.3278, val loss 1.3949
MLP Attention model: step 1000: train loss 1.3308, val loss 1.4139
Original model: step 1050: train loss 1.3002, val loss 1.3815
MLP Attention model: step 1050: train loss 1.3119, val loss 1.3944
Original model: step 1100: train loss 1.2895, val loss 1.3692
MLP Attention model: step 1100: train loss 1.2930, val loss 1.3878
Original model: step 1150: train loss 1.2662, val loss 1.3507
MLP Attention model: step 1150: train loss 1.2787, val loss 1.3712
Original model: step 1200: train loss 1.2424, val loss 1.3444
MLP Attention model: step 1200: train loss 1.2723, val loss 1.3753
Original model: step 1250: train loss 1.2330, val loss 1.3379
MLP Attention model: step 1250: train loss 1.2553, val loss 1.3589
Original model: step 1300: train loss 1.2146, val loss 1.3266
MLP Attention model: step 1300: train loss 1.2425, val loss 1.3568
Original model: step 1350: train loss 1.2004, val loss 1.3232
MLP Attention model: step 1350: train loss 1.2332, val loss 1.3485
Original model: step 1400: train loss 1.1861, val loss 1.3123
MLP Attention model: step 1400: train loss 1.2221, val loss 1.3452
Original model: step 1450: train loss 1.1730, val loss 1.3111
MLP Attention model: step 1450: train loss 1.2041, val loss 1.3383
Original model: step 1500: train loss 1.1664, val loss 1.3120
MLP Attention model: step 1500: train loss 1.1992, val loss 1.3327
Original model: step 1550: train loss 1.1515, val loss 1.2969
MLP Attention model: step 1550: train loss 1.1861, val loss 1.3265
Original model: step 1600: train loss 1.1430, val loss 1.2985
MLP Attention model: step 1600: train loss 1.1780, val loss 1.3282
Original model: step 1650: train loss 1.1357, val loss 1.3011
MLP Attention model: step 1650: train loss 1.1705, val loss 1.3214
Original model: step 1700: train loss 1.1164, val loss 1.2858
MLP Attention model: step 1700: train loss 1.1572, val loss 1.3132
Original model: step 1750: train loss 1.1060, val loss 1.2813
MLP Attention model: step 1750: train loss 1.1427, val loss 1.3113
Original model: step 1800: train loss 1.0949, val loss 1.2771
MLP Attention model: step 1800: train loss 1.1411, val loss 1.3074
Original model: step 1850: train loss 1.0900, val loss 1.2780
MLP Attention model: step 1850: train loss 1.1302, val loss 1.3074
Original model: step 1900: train loss 1.0877, val loss 1.2824
MLP Attention model: step 1900: train loss 1.1207, val loss 1.3041
Original model: step 1950: train loss 1.0699, val loss 1.2678
MLP Attention model: step 1950: train loss 1.1209, val loss 1.3081
Original model: step 2000: train loss 1.0610, val loss 1.2740
MLP Attention model: step 2000: train loss 1.1051, val loss 1.2971
Original model: step 2050: train loss 1.0480, val loss 1.2637
MLP Attention model: step 2050: train loss 1.0976, val loss 1.2970
Original model: step 2100: train loss 1.0423, val loss 1.2686
MLP Attention model: step 2100: train loss 1.0903, val loss 1.2950
Original model: step 2150: train loss 1.0364, val loss 1.2701
MLP Attention model: step 2150: train loss 1.0857, val loss 1.2997
Original model: step 2200: train loss 1.0254, val loss 1.2640
MLP Attention model: step 2200: train loss 1.0749, val loss 1.2864
Original model: step 2250: train loss 1.0229, val loss 1.2642
MLP Attention model: step 2250: train loss 1.0661, val loss 1.2905
Original model: step 2300: train loss 1.0124, val loss 1.2640
MLP Attention model: step 2300: train loss 1.0584, val loss 1.2879
Original model: step 2350: train loss 1.0041, val loss 1.2653
MLP Attention model: step 2350: train loss 1.0566, val loss 1.2876
Original model: step 2400: train loss 0.9941, val loss 1.2586
MLP Attention model: step 2400: train loss 1.0449, val loss 1.2877
Original model: step 2450: train loss 0.9873, val loss 1.2638
MLP Attention model: step 2450: train loss 1.0477, val loss 1.2924
Original model: step 2500: train loss 0.9837, val loss 1.2644
MLP Attention model: step 2500: train loss 1.0320, val loss 1.2830
Original model: step 2550: train loss 0.9708, val loss 1.2639
MLP Attention model: step 2550: train loss 1.0316, val loss 1.2806
Original model: step 2600: train loss 0.9615, val loss 1.2589
MLP Attention model: step 2600: train loss 1.0219, val loss 1.2839
Original model: step 2650: train loss 0.9631, val loss 1.2659
MLP Attention model: step 2650: train loss 1.0122, val loss 1.2830
Original model: step 2700: train loss 0.9551, val loss 1.2584
MLP Attention model: step 2700: train loss 1.0124, val loss 1.2878
Original model: step 2750: train loss 0.9423, val loss 1.2651
MLP Attention model: step 2750: train loss 1.0029, val loss 1.2811
Original model: step 2800: train loss 0.9450, val loss 1.2713
MLP Attention model: step 2800: train loss 0.9915, val loss 1.2772
Original model: step 2850: train loss 0.9325, val loss 1.2584
MLP Attention model: step 2850: train loss 0.9863, val loss 1.2759
Original model: step 2900: train loss 0.9291, val loss 1.2681
MLP Attention model: step 2900: train loss 0.9851, val loss 1.2784
Original model: step 2950: train loss 0.9233, val loss 1.2705
MLP Attention model: step 2950: train loss 0.9753, val loss 1.2745
Original model: step 2999: train loss 0.9120, val loss 1.2696
MLP Attention model: step 2999: train loss 0.9743, val loss 1.2843
